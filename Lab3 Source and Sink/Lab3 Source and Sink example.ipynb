{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4eba7-c68d-4f5b-be01-f7afc5233150",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"D:\\\\anmv2\\\\Environment\\\\Jdk1.8\"\n",
    "os.environ[\"SPARK_HOME\"] = \"D:\\\\anmv2\\\\Environment\\\\Spark\\\\spark-3.5.3-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:\\\\anmv2\\\\Environment\\\\Hadoop\\\\hadoop\"\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1661f9-9502-4b49-b3db-9fb2c8d2b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DateType, StringType, IntegerType\n",
    "\n",
    "# Tạo SparkConf và cấu hình các tham số\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local') \\\n",
    "    .setAppName('Lab3 Source and Sink') \n",
    "\n",
    "# Tạo SparkSession từ SparkConf\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "# Lấy SparkContext từ SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4cbae-a00d-4268-9d19-2864105d9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = 'data_lab3.1_source_and_sink.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519443c-80b0-427b-a8a3-044c0f0c0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "# Khởi tạo Schema bằng StructType\n",
    "schema = StructType([\n",
    "    StructField(\"FL_DATE\", DateType(), True),\n",
    "    StructField(\"OP_CARRIER\", StringType(), True),\n",
    "    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), True),\n",
    "    StructField(\"ORIGIN\", StringType(), True),\n",
    "    StructField(\"ORIGIN_CITY_NAME\", StringType(), True),\n",
    "    StructField(\"DEST\", StringType(), True),\n",
    "    StructField(\"DEST_CITY_NAME\", StringType(), True),\n",
    "    StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
    "    StructField(\"DEP_TIME\", IntegerType(), True),\n",
    "    StructField(\"WHEELS_ON\", IntegerType(), True),\n",
    "    StructField(\"TAXI_IN\", IntegerType(), True),\n",
    "    StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
    "    StructField(\"ARR_TIME\", IntegerType(), True),\n",
    "    StructField(\"CANCELLED\", IntegerType(), True),\n",
    "    StructField(\"DISTANCE\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .load(DATA_IN)\n",
    "\n",
    "print(\"Schema create by Struct\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66fec3-cb0c-47e5-b21b-f2fea867199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "# Khởi tạo Schema bằng DDL\n",
    "str_schema = \"\"\"\n",
    "    FL_DATE DATE,\n",
    "    OP_CARRIER STRING,\n",
    "    OP_CARRIER_FL_NUM INT,\n",
    "    ORIGIN STRING,\n",
    "    ORIGIN_CITY_NAME STRING,\n",
    "    DEST STRING,\n",
    "    DEST_CITY_NAME STRING,\n",
    "    CRS_DEP_TIME INT,\n",
    "    DEP_TIME INT,\n",
    "    WHEELS_ON INT,\n",
    "    TAXI_IN INT,\n",
    "    CRS_ARR_TIME INT,\n",
    "    ARR_TIME INT,\n",
    "    CANCELLED INT,\n",
    "    DISTANCE INT\n",
    "\"\"\"\n",
    "\n",
    "print(\"Schema create by DDL string\")\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(str_schema) \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .load(DATA_IN)\n",
    "\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4b881-830c-4365-9b8d-5396a5e75af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện check partitions hiện tại\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "df.groupBy(spark_partition_id()).count().show()\n",
    "print(\"Num Partitions before: \" + str(df.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408450a4-4b92-4e2a-a303-c80df6788159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện repartition thành 5 \n",
    "\n",
    "partitionedDF = df.repartition(5)\n",
    "print(\"Num Partitions after: \" + str(partitionedDF.rdd.getNumPartitions()))\n",
    "partitionedDF.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94314d27-7558-46e6-aae9-2b7c73648883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write df sau khi partition:\n",
    "partitionedDF.write \\\n",
    "    .format(\"json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"lab3_output.json\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
